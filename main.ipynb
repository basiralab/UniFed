{"cells":[{"cell_type":"markdown","metadata":{"id":"5flL4-j_plA2"},"source":["# Multi-task Sequential Federated Learning on MedMNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9513,"status":"ok","timestamp":1708391495782,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"gJEvyIR-GPwo","outputId":"62219a6c-0398-4305-c067-9518c7d54e43"},"outputs":[],"source":["! pip install medmnist\n","! pip3 install pyyaml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3381,"status":"ok","timestamp":1708391499159,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"djBHID0dlsX6","outputId":"449003a3-41fc-4a8b-e03d-504710da775c"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708391499160,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"rRHSj6tSlvs6","outputId":"85cc372b-5fa6-4a46-db25-41937c926743"},"outputs":[],"source":["cd /content/drive/MyDrive/FederatedLearning/MICCAI2024/"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4201,"status":"ok","timestamp":1708391503357,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"CgS_R2FEF4wB"},"outputs":[],"source":["# Import libraries\n","import medmnist\n","from medmnist import INFO\n","\n","import random\n","from tqdm import tqdm\n","import time\n","import numpy as np\n","from statistics import mean\n","import PIL\n","from itertools import chain\n","import yaml\n","\n","import torch\n","import torch.nn as nn\n","\n","# Models\n","from models.CNN import CNN5\n","from models.VGG11 import VGG11\n","from models.ResNet18 import ResNet_18, Block\n","from models.SimCNN import SimCNN\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","# Train and Test/validation\n","from utils.Training import Train\n","from utils.Evaluation import Evaluator\n","\n","# data\n","from utils.StratifiedDatasets import StratifiedData\n","from utils.IIDDatasets import IIDData\n","\n","# import EarlyStopping\n","from utils.pytorchtools import EarlyStopping\n","\n","# making code reproducible\n","from utils.seedeverything import seedevrything"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n","os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1708391508234,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"FOXzzHEnXZBC","outputId":"919c4019-6ca9-4d80-a287-9facded2a3f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","\n"]}],"source":["# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391508234,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"9-zwIFP6l5Dr","outputId":"3c7c57b4-3d4e-4b28-9d14-f1b5c37b1876"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random seed set as 42\n"]}],"source":["# Setting Up Random Seeds In PyTorch\n","seedevrything.set_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"EhBdxPIOjpDr"},"source":["### Dataset loading"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1708391508234,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"oY8QVJTkGN6M","outputId":"c3cf6b05-0c5b-4fb0-faea-a27c3bcbcd1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["MedMNIST v3.0.1 @ https://github.com/MedMNIST/MedMNIST/\n"]}],"source":["print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1708391508234,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"e5CKwJ3_HrFe","outputId":"85451f3d-4976-4317-f390-d73b16aef21a"},"outputs":[{"data":{"text/plain":["{'pathmnist': {'python_class': 'PathMNIST',\n","  'description': 'The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/pathmnist.npz?download=1',\n","  'MD5': 'a8b06965200029087d5bd730944a56c1',\n","  'url_64': 'https://zenodo.org/records/10519652/files/pathmnist_64.npz?download=1',\n","  'MD5_64': '55aa9c1e0525abe5a6b9d8343a507616',\n","  'url_128': 'https://zenodo.org/records/10519652/files/pathmnist_128.npz?download=1',\n","  'MD5_128': 'ac42d08fb904d92c244187169d1fd1d9',\n","  'url_224': 'https://zenodo.org/records/10519652/files/pathmnist_224.npz?download=1',\n","  'MD5_224': '2c51a510bcdc9cf8ddb2af93af1eadec',\n","  'task': 'multi-class',\n","  'label': {'0': 'adipose',\n","   '1': 'background',\n","   '2': 'debris',\n","   '3': 'lymphocytes',\n","   '4': 'mucus',\n","   '5': 'smooth muscle',\n","   '6': 'normal colon mucosa',\n","   '7': 'cancer-associated stroma',\n","   '8': 'colorectal adenocarcinoma epithelium'},\n","  'n_channels': 3,\n","  'n_samples': {'train': 89996, 'val': 10004, 'test': 7180},\n","  'license': 'CC BY 4.0'},\n"," 'chestmnist': {'python_class': 'ChestMNIST',\n","  'description': 'The ChestMNIST is based on the NIH-ChestXray14 dataset, a dataset comprising 112,120 frontal-view X-Ray images of 30,805 unique patients with the text-mined 14 disease labels, which could be formulized as a multi-label binary-class classification task. We use the official data split, and resize the source images of 1×1024×1024 into 1×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/chestmnist.npz?download=1',\n","  'MD5': '02c8a6516a18b556561a56cbdd36c4a8',\n","  'url_64': 'https://zenodo.org/records/10519652/files/chestmnist_64.npz?download=1',\n","  'MD5_64': '9de6cd0b934ebb5b7426cfba5efbae16',\n","  'url_128': 'https://zenodo.org/records/10519652/files/chestmnist_128.npz?download=1',\n","  'MD5_128': 'db107e5590b27930b62dbaf558aebee3',\n","  'url_224': 'https://zenodo.org/records/10519652/files/chestmnist_224.npz?download=1',\n","  'MD5_224': '45bd33e6f06c3e8cdb481c74a89152aa',\n","  'task': 'multi-label, binary-class',\n","  'label': {'0': 'atelectasis',\n","   '1': 'cardiomegaly',\n","   '2': 'effusion',\n","   '3': 'infiltration',\n","   '4': 'mass',\n","   '5': 'nodule',\n","   '6': 'pneumonia',\n","   '7': 'pneumothorax',\n","   '8': 'consolidation',\n","   '9': 'edema',\n","   '10': 'emphysema',\n","   '11': 'fibrosis',\n","   '12': 'pleural',\n","   '13': 'hernia'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 78468, 'val': 11219, 'test': 22433},\n","  'license': 'CC BY 4.0'},\n"," 'dermamnist': {'python_class': 'DermaMNIST',\n","  'description': 'The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/dermamnist.npz?download=1',\n","  'MD5': '0744692d530f8e62ec473284d019b0c7',\n","  'url_64': 'https://zenodo.org/records/10519652/files/dermamnist_64.npz?download=1',\n","  'MD5_64': 'b70a2f5635c6199aeaa28c31d7202e1f',\n","  'url_128': 'https://zenodo.org/records/10519652/files/dermamnist_128.npz?download=1',\n","  'MD5_128': '2defd784463fa5243564e855ed717de1',\n","  'url_224': 'https://zenodo.org/records/10519652/files/dermamnist_224.npz?download=1',\n","  'MD5_224': '8974907d8e169bef5f5b96bc506ae45d',\n","  'task': 'multi-class',\n","  'label': {'0': 'actinic keratoses and intraepithelial carcinoma',\n","   '1': 'basal cell carcinoma',\n","   '2': 'benign keratosis-like lesions',\n","   '3': 'dermatofibroma',\n","   '4': 'melanoma',\n","   '5': 'melanocytic nevi',\n","   '6': 'vascular lesions'},\n","  'n_channels': 3,\n","  'n_samples': {'train': 7007, 'val': 1003, 'test': 2005},\n","  'license': 'CC BY-NC 4.0'},\n"," 'octmnist': {'python_class': 'OCTMNIST',\n","  'description': 'The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−1,536)×(277−512). We center-crop the images and resize them into 1×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/octmnist.npz?download=1',\n","  'MD5': 'c68d92d5b585d8d81f7112f81e2d0842',\n","  'url_64': 'https://zenodo.org/records/10519652/files/octmnist_64.npz?download=1',\n","  'MD5_64': 'e229e9440236b774d9f0dfef9d07bdaf',\n","  'url_128': 'https://zenodo.org/records/10519652/files/octmnist_128.npz?download=1',\n","  'MD5_128': '0a97e76651ace45c5d943ee3f65b63ae',\n","  'url_224': 'https://zenodo.org/records/10519652/files/octmnist_224.npz?download=1',\n","  'MD5_224': 'abc493b6d529d5de7569faaef2773ba3',\n","  'task': 'multi-class',\n","  'label': {'0': 'choroidal neovascularization',\n","   '1': 'diabetic macular edema',\n","   '2': 'drusen',\n","   '3': 'normal'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 97477, 'val': 10832, 'test': 1000},\n","  'license': 'CC BY 4.0'},\n"," 'pneumoniamnist': {'python_class': 'PneumoniaMNIST',\n","  'description': 'The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1',\n","  'MD5': '28209eda62fecd6e6a2d98b1501bb15f',\n","  'url_64': 'https://zenodo.org/records/10519652/files/pneumoniamnist_64.npz?download=1',\n","  'MD5_64': '8f4eceb4ccffa70c672198ea285246c6',\n","  'url_128': 'https://zenodo.org/records/10519652/files/pneumoniamnist_128.npz?download=1',\n","  'MD5_128': '05b46931834c231683c68f40c47b2971',\n","  'url_224': 'https://zenodo.org/records/10519652/files/pneumoniamnist_224.npz?download=1',\n","  'MD5_224': 'd6a3c71de1b945ea11211b03746c1fe1',\n","  'task': 'binary-class',\n","  'label': {'0': 'normal', '1': 'pneumonia'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 4708, 'val': 524, 'test': 624},\n","  'license': 'CC BY 4.0'},\n"," 'retinamnist': {'python_class': 'RetinaMNIST',\n","  'description': 'The RetinaMNIST is based on the DeepDRiD challenge, which provides a dataset of 1,600 retina fundus images. The task is ordinal regression for 5-level grading of diabetic retinopathy severity. We split the source training set with a ratio of 9:1 into training and validation set, and use the source validation set as the test set. The source images of 3×1,736×1,824 are center-cropped and resized into 3×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/retinamnist.npz?download=1',\n","  'MD5': 'bd4c0672f1bba3e3a89f0e4e876791e4',\n","  'url_64': 'https://zenodo.org/records/10519652/files/retinamnist_64.npz?download=1',\n","  'MD5_64': 'afda852cc34dcda56f86ad2b2457dbcc',\n","  'url_128': 'https://zenodo.org/records/10519652/files/retinamnist_128.npz?download=1',\n","  'MD5_128': 'e48e916a24454daf90583d4e6efb1a18',\n","  'url_224': 'https://zenodo.org/records/10519652/files/retinamnist_224.npz?download=1',\n","  'MD5_224': 'eae7e3b6f3fcbda4ae613ebdcbe35348',\n","  'task': 'ordinal-regression',\n","  'label': {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4'},\n","  'n_channels': 3,\n","  'n_samples': {'train': 1080, 'val': 120, 'test': 400},\n","  'license': 'CC BY 4.0'},\n"," 'breastmnist': {'python_class': 'BreastMNIST',\n","  'description': 'The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/breastmnist.npz?download=1',\n","  'MD5': '750601b1f35ba3300ea97c75c52ff8f6',\n","  'url_64': 'https://zenodo.org/records/10519652/files/breastmnist_64.npz?download=1',\n","  'MD5_64': '742edef2a1fd1524b2efff4bd7ba9364',\n","  'url_128': 'https://zenodo.org/records/10519652/files/breastmnist_128.npz?download=1',\n","  'MD5_128': '363e4b3f8d712e9b5de15470a2aaadf1',\n","  'url_224': 'https://zenodo.org/records/10519652/files/breastmnist_224.npz?download=1',\n","  'MD5_224': 'b56378a6eefa9fed602bb16d192d4c8b',\n","  'task': 'binary-class',\n","  'label': {'0': 'malignant', '1': 'normal, benign'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 546, 'val': 78, 'test': 156},\n","  'license': 'CC BY 4.0'},\n"," 'bloodmnist': {'python_class': 'BloodMNIST',\n","  'description': 'The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/bloodmnist.npz?download=1',\n","  'MD5': '7053d0359d879ad8a5505303e11de1dc',\n","  'url_64': 'https://zenodo.org/records/10519652/files/bloodmnist_64.npz?download=1',\n","  'MD5_64': '2b94928a2ae4916078ca51e05b6b800b',\n","  'url_128': 'https://zenodo.org/records/10519652/files/bloodmnist_128.npz?download=1',\n","  'MD5_128': 'adace1e0ed228fccda1f39692059dd4c',\n","  'url_224': 'https://zenodo.org/records/10519652/files/bloodmnist_224.npz?download=1',\n","  'MD5_224': 'b718ff6835fcbdb22ba9eacccd7b2601',\n","  'task': 'multi-class',\n","  'label': {'0': 'basophil',\n","   '1': 'eosinophil',\n","   '2': 'erythroblast',\n","   '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)',\n","   '4': 'lymphocyte',\n","   '5': 'monocyte',\n","   '6': 'neutrophil',\n","   '7': 'platelet'},\n","  'n_channels': 3,\n","  'n_samples': {'train': 11959, 'val': 1712, 'test': 3421},\n","  'license': 'CC BY 4.0'},\n"," 'tissuemnist': {'python_class': 'TissueMNIST',\n","  'description': 'We use the BBBC051, available from the Broad Bioimage Benchmark Collection. The dataset contains 236,386 human kidney cortex cells, segmented from 3 reference tissue specimens and organized into 8 categories. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. Each gray-scale image is 32×32×7 pixels, where 7 denotes 7 slices. We take maximum values across the slices and resize them into 28×28 gray-scale images.',\n","  'url': 'https://zenodo.org/records/10519652/files/tissuemnist.npz?download=1',\n","  'MD5': 'ebe78ee8b05294063de985d821c1c34b',\n","  'url_64': 'https://zenodo.org/records/10519652/files/tissuemnist_64.npz?download=1',\n","  'MD5_64': '123ece2eba09d0aa5d698fda57103344',\n","  'url_128': 'https://zenodo.org/records/10519652/files/tissuemnist_128.npz?download=1',\n","  'MD5_128': '61b955355d7425a89687b06cca3ce0c2',\n","  'url_224': 'https://zenodo.org/records/10519652/files/tissuemnist_224.npz?download=1',\n","  'MD5_224': 'b077128c4a949f0a4eb01517f9037b9c',\n","  'task': 'multi-class',\n","  'label': {'0': 'Collecting Duct, Connecting Tubule',\n","   '1': 'Distal Convoluted Tubule',\n","   '2': 'Glomerular endothelial cells',\n","   '3': 'Interstitial endothelial cells',\n","   '4': 'Leukocytes',\n","   '5': 'Podocytes',\n","   '6': 'Proximal Tubule Segments',\n","   '7': 'Thick Ascending Limb'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 165466, 'val': 23640, 'test': 47280},\n","  'license': 'CC BY 4.0'},\n"," 'organamnist': {'python_class': 'OrganAMNIST',\n","  'description': 'The OrganAMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Axial (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in axial views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/organamnist.npz?download=1',\n","  'MD5': '68e3f8846a6bd62f0c9bf841c0d9eacc',\n","  'url_64': 'https://zenodo.org/records/10519652/files/organamnist_64.npz?download=1',\n","  'MD5_64': '2dcccc29b88e6da5a01161ef20cda288',\n","  'url_128': 'https://zenodo.org/records/10519652/files/organamnist_128.npz?download=1',\n","  'MD5_128': 'eeae80d0a227a8d099027e1b3cfd3b60',\n","  'url_224': 'https://zenodo.org/records/10519652/files/organamnist_224.npz?download=1',\n","  'MD5_224': '50747347e05c87dd3aaf92c49f9f3170',\n","  'task': 'multi-class',\n","  'label': {'0': 'bladder',\n","   '1': 'femur-left',\n","   '2': 'femur-right',\n","   '3': 'heart',\n","   '4': 'kidney-left',\n","   '5': 'kidney-right',\n","   '6': 'liver',\n","   '7': 'lung-left',\n","   '8': 'lung-right',\n","   '9': 'pancreas',\n","   '10': 'spleen'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 34561, 'val': 6491, 'test': 17778},\n","  'license': 'CC BY 4.0'},\n"," 'organcmnist': {'python_class': 'OrganCMNIST',\n","  'description': 'The OrganCMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Coronal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in coronal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/organcmnist.npz?download=1',\n","  'MD5': 'b9ceb9546e10131b32923c5bbeaea2b1',\n","  'url_64': 'https://zenodo.org/records/10519652/files/organcmnist_64.npz?download=1',\n","  'MD5_64': '3ce34a8724ea6f548e6db4744d03b6a9',\n","  'url_128': 'https://zenodo.org/records/10519652/files/organcmnist_128.npz?download=1',\n","  'MD5_128': '773c1f009daa3fe5d9a2a201b2a7ed94',\n","  'url_224': 'https://zenodo.org/records/10519652/files/organcmnist_224.npz?download=1',\n","  'MD5_224': '050f5e875dc056f6768abf94ec9995d1',\n","  'task': 'multi-class',\n","  'label': {'0': 'bladder',\n","   '1': 'femur-left',\n","   '2': 'femur-right',\n","   '3': 'heart',\n","   '4': 'kidney-left',\n","   '5': 'kidney-right',\n","   '6': 'liver',\n","   '7': 'lung-left',\n","   '8': 'lung-right',\n","   '9': 'pancreas',\n","   '10': 'spleen'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 12975, 'val': 2392, 'test': 8216},\n","  'license': 'CC BY 4.0'},\n"," 'organsmnist': {'python_class': 'OrganSMNIST',\n","  'description': 'The OrganSMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Sagittal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in sagittal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/organsmnist.npz?download=1',\n","  'MD5': '9ab87b696fb54e2a387ebe992d6ed5f1',\n","  'url_64': 'https://zenodo.org/records/10519652/files/organsmnist_64.npz?download=1',\n","  'MD5_64': '53a6d115339d874c25e309a994ff46d3',\n","  'url_128': 'https://zenodo.org/records/10519652/files/organsmnist_128.npz?download=1',\n","  'MD5_128': 'ded0c5fa01a95dc4978b956f613e9b8e',\n","  'url_224': 'https://zenodo.org/records/10519652/files/organsmnist_224.npz?download=1',\n","  'MD5_224': 'b354719e553fbbb2513d5533f52a4cb1',\n","  'task': 'multi-class',\n","  'label': {'0': 'bladder',\n","   '1': 'femur-left',\n","   '2': 'femur-right',\n","   '3': 'heart',\n","   '4': 'kidney-left',\n","   '5': 'kidney-right',\n","   '6': 'liver',\n","   '7': 'lung-left',\n","   '8': 'lung-right',\n","   '9': 'pancreas',\n","   '10': 'spleen'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 13932, 'val': 2452, 'test': 8827},\n","  'license': 'CC BY 4.0'},\n"," 'organmnist3d': {'python_class': 'OrganMNIST3D',\n","  'description': 'The source of the OrganMNIST3D is the same as that of the Organ{A,C,S}MNIST. Instead of 2D images, we directly use the 3D bounding boxes and process the images into 28×28×28 to perform multi-class classification of 11 body organs. The same 115 and 16 CT scans as the Organ{A,C,S}MNIST from the source training set are used as training and validation set, respectively, and the same 70 CT scans as the Organ{A,C,S}MNIST from the source test set are treated as the test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/organmnist3d.npz?download=1',\n","  'MD5': 'a0c5a1ff56af4f155c46d46fbb45a2fe',\n","  'url_64': 'https://zenodo.org/records/10519652/files/organmnist3d_64.npz?download=1',\n","  'MD5_64': '58a2205adf14a9d0a189cb06dc78bf10',\n","  'task': 'multi-class',\n","  'label': {'0': 'liver',\n","   '1': 'kidney-right',\n","   '2': 'kidney-left',\n","   '3': 'femur-right',\n","   '4': 'femur-left',\n","   '5': 'bladder',\n","   '6': 'heart',\n","   '7': 'lung-right',\n","   '8': 'lung-left',\n","   '9': 'spleen',\n","   '10': 'pancreas'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 971, 'val': 161, 'test': 610},\n","  'license': 'CC BY 4.0'},\n"," 'nodulemnist3d': {'python_class': 'NoduleMNIST3D',\n","  'description': 'The NoduleMNIST3D is based on the LIDC-IDRI, a large public lung nodule dataset, containing images from thoracic CT scans. The dataset is designed for both lung nodule segmentation and 5-level malignancy classification task. To perform binary classification, we categorize cases with malignancy level 1/2 into negative class and 4/5 into positive class, ignoring the cases with malignancy level 3. We split the source dataset with a ratio of 7:1:2 into training, validation and test set, and center-crop the spatially normalized images (with a spacing of 1mm×1mm×1mm) into 28×28×28.',\n","  'url': 'https://zenodo.org/records/10519652/files/nodulemnist3d.npz?download=1',\n","  'MD5': '8755a7e9e05a4d9ce80a24c3e7a256f3',\n","  'url_64': 'https://zenodo.org/records/10519652/files/nodulemnist3d_64.npz?download=1',\n","  'MD5_64': 'c47c5b7d457bf6332200d2ea6d64ecd8',\n","  'task': 'binary-class',\n","  'label': {'0': 'benign', '1': 'malignant'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 1158, 'val': 165, 'test': 310},\n","  'license': 'CC BY 4.0'},\n"," 'adrenalmnist3d': {'python_class': 'AdrenalMNIST3D',\n","  'description': 'The AdrenalMNIST3D is a new 3D shape classification dataset, consisting of shape masks from 1,584 left and right adrenal glands (i.e., 792 patients). Collected from Zhongshan Hospital Affiliated to Fudan University, each 3D shape of adrenal gland is annotated by an expert endocrinologist using abdominal computed tomography (CT), together with a binary classification label of normal adrenal gland or adrenal mass. Considering patient privacy, we do not provide the source CT scans, but the real 3D shapes of adrenal glands and their classification labels. We calculate the center of adrenal and resize the center-cropped 64mm×64mm×64mm volume into 28×28×28. The dataset is randomly split into training/validation/test set of 1,188/98/298 on a patient level.',\n","  'url': 'https://zenodo.org/records/10519652/files/adrenalmnist3d.npz?download=1',\n","  'MD5': 'bbd3c5a5576322bc4cdfea780653b1ce',\n","  'url_64': 'https://zenodo.org/records/10519652/files/adrenalmnist3d_64.npz?download=1',\n","  'MD5_64': '17721accfe9fb005146a47d33bc54b2f',\n","  'task': 'binary-class',\n","  'label': {'0': 'normal', '1': 'hyperplasia'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 1188, 'val': 98, 'test': 298},\n","  'license': 'CC BY 4.0'},\n"," 'fracturemnist3d': {'python_class': 'FractureMNIST3D',\n","  'description': 'The FractureMNIST3D is based on the RibFrac Dataset, containing around 5,000 rib fractures from 660 computed tomography 153 (CT) scans. The dataset organizes detected rib fractures into 4 clinical categories (i.e., buckle, nondisplaced, displaced, and segmental rib fractures). As we use low-resolution images, we disregard segmental rib fractures and classify 3 types of rib fractures (i.e., buckle, nondisplaced, and displaced). For each annotated fracture area, we calculate its center and resize the center-cropped 64mm×64mm×64mm image into 28×28×28. The official split of training, validation and test set is used.',\n","  'url': 'https://zenodo.org/records/10519652/files/fracturemnist3d.npz?download=1',\n","  'MD5': '6aa7b0143a6b42da40027a9dda61302f',\n","  'url_64': 'https://zenodo.org/records/10519652/files/fracturemnist3d_64.npz?download=1',\n","  'MD5_64': 'f01d7e6316aedf4210da0da5b7437b42',\n","  'task': 'multi-class',\n","  'label': {'0': 'buckle rib fracture',\n","   '1': 'nondisplaced rib fracture',\n","   '2': 'displaced rib fracture'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 1027, 'val': 103, 'test': 240},\n","  'license': 'CC BY 4.0'},\n"," 'vesselmnist3d': {'python_class': 'VesselMNIST3D',\n","  'description': 'The VesselMNIST3D is based on an open-access 3D intracranial aneurysm dataset, IntrA, containing 103 3D models (meshes) of entire brain vessels collected by reconstructing MRA images. 1,694 healthy vessel segments and 215 aneurysm segments are generated automatically from the complete models. We fix the non-watertight mesh with PyMeshFix and voxelize the watertight mesh with trimesh into 28×28×28 voxels. We split the source dataset with a ratio of 7:1:2 into training, validation and test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/vesselmnist3d.npz?download=1',\n","  'MD5': 'b41fd4f7e7e2feedddb201585ecafa1b',\n","  'url_64': 'https://zenodo.org/records/10519652/files/vesselmnist3d_64.npz?download=1',\n","  'MD5_64': '6bb274a8846e1097066dcd64e2c4520f',\n","  'task': 'binary-class',\n","  'label': {'0': 'vessel', '1': 'aneurysm'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 1335, 'val': 191, 'test': 382},\n","  'license': 'CC BY 4.0'},\n"," 'synapsemnist3d': {'python_class': 'SynapseMNIST3D',\n","  'description': 'The SynapseMNIST3D is a new 3D volume dataset to classify whether a synapse is excitatory or inhibitory. It uses a 3D image volume of an adult rat acquired by a multi-beam scanning electron microscope. The original data is of the size 100×100×100um^3 and the resolution 8×8×30nm^3, where a (30um)^3 sub-volume was used in the MitoEM dataset with dense 3D mitochondria instance segmentation labels. Three neuroscience experts segment a pyramidal neuron within the whole volume and proofread all the synapses on this neuron with excitatory/inhibitory labels. For each labeled synaptic location, we crop a 3D volume of 1024×1024×1024nm^3 and resize it into 28×28×28 voxels. Finally, the dataset is randomly split with a ratio of 7:1:2 into training, validation and test set.',\n","  'url': 'https://zenodo.org/records/10519652/files/synapsemnist3d.npz?download=1',\n","  'MD5': '1235b78a3cd6280881dd7850a78eadb6',\n","  'url_64': 'https://zenodo.org/records/10519652/files/synapsemnist3d_64.npz?download=1',\n","  'MD5_64': '43bd14ebf3af9d3dd072446fedc14d5e',\n","  'task': 'binary-class',\n","  'label': {'0': 'inhibitory synapse', '1': 'excitatory synapse'},\n","  'n_channels': 1,\n","  'n_samples': {'train': 1230, 'val': 177, 'test': 352},\n","  'license': 'CC BY 4.0'}}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# getting all data flags in MedMNIST\n","medmnist.INFO"]},{"cell_type":"markdown","metadata":{"id":"4PA7DfIPmBq7"},"source":["## Load configs"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1708391510153,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"HPVk5OTQl_sQ"},"outputs":[],"source":["# load configs data\n","with open(\"configs.yaml\", 'r') as configs:\n","    configs = yaml.safe_load(configs)"]},{"cell_type":"markdown","metadata":{"id":"nih6aN95mUUK"},"source":["### Dataset loading"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708391511568,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"6iDm_mnVmLu-"},"outputs":[],"source":["data_flags = configs['dataset']['data_flags']"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708391511569,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"bAaI6YVmiAwZ","outputId":"c7ca0f3c-a6c1-4974-aa37-866af03180db"},"outputs":[{"name":"stdout","output_type":"stream","text":["The numebr of labels in all datasets is: 23\n"]}],"source":["n_classes = 0\n","for dataset in data_flags:\n","  n_classes += len(medmnist.INFO[dataset]['label'])\n","print(f\"The numebr of labels in all datasets is: {n_classes}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7469,"status":"ok","timestamp":1708391519034,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"4-BlVCj9Gnai","outputId":"d6f7b140-6677-49db-96f3-1a1317d57126"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: /home/srahmani/.medmnist/octmnist.npz\n","Using downloaded and verified file: /home/srahmani/.medmnist/organamnist.npz\n","Using downloaded and verified file: /home/srahmani/.medmnist/tissuemnist.npz\n"]}],"source":["# get an instance from Data class\n","amount = 8000\n","# client_splitting_method\n","cmp = None\n","\n","# SAMPLING = ['iid', 'noniid']\n","SAMPLING = \"noniid\"\n","\n","if SAMPLING == 'noniid':\n","    # No-IID sampling\n","    dataset = StratifiedData(data_flags = data_flags, amount = amount, num_clients = 8)\n","    cmp = \"ordered\"\n","elif SAMPLING == 'iid':\n","    # IID sampling\n","    dataset = IIDData(data_flags = data_flags, amount = amount, split_method=\"stratify\")\n","    cmp = 'equal'\n","\n","# get the server data\n","train_dl_server, valid_dl_server, test_dl_server = dataset.get_server_data()\n","# get clients data\n","x_train_dict, y_train_dict, x_valid_dict, y_valid_dict, x_test_dict, y_test_dict = dataset.get_clients_data(client_splitting_method=cmp)"]},{"cell_type":"markdown","metadata":{"id":"BgW6ZSj8jd8D"},"source":["### Classification Model"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1708391519034,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"Zm06e-iYdL0K"},"outputs":[],"source":["def get_model(model_name):\n","  if model_name == 'cnn5':\n","    return CNN5(num_classes=n_classes)\n","  if model_name == 'vgg11':\n","    return VGG11(num_classes=n_classes)\n","  if model_name == 'simcnn':\n","    return SimCNN(num_classes=n_classes)\n","  elif model_name == 'resnet18':\n","    return ResNet_18(num_classes=n_classes)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519034,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"1GMR88-eXZBE"},"outputs":[],"source":["trainer = Train(n_classes)\n","validator = Evaluator(n_classes)"]},{"cell_type":"markdown","metadata":{"id":"BV89wt0akC3C"},"source":["## Functions for Federated"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"h4nW_NIQj_5W"},"outputs":[],"source":["def create_model_optimizer_criterion_dict(number_of_samples):\n","  '''\n","  creates three dictionaries of models, optimizers, and criteria for all the clients!\n","\n","  @param: number_of_samples  --> number of clients\n","\n","  return: model_dict, optimizer_dict, criterion_dict\n","  '''\n","  model_dict = dict()\n","  optimizer_dict= dict()\n","  criterion_dict = dict()\n","\n","  for i in range(number_of_samples):\n","    model_name=\"model\"+str(i)\n","    model_info = get_model(model_name=MODEL)\n","    model_info.to(device)\n","    model_dict.update({model_name : model_info })\n","\n","    optimizer_name = \"optimizer\"+str(i)\n","    optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n","    optimizer_dict.update({optimizer_name : optimizer_info })\n","\n","    criterion_name = \"criterion\"+str(i)\n","    criterion_info = nn.CrossEntropyLoss()\n","    criterion_dict.update({criterion_name : criterion_info})\n","\n","  return model_dict, optimizer_dict, criterion_dict"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"BXAGxZ-bLGVf"},"outputs":[],"source":["def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples):\n","  '''\n","  sends main_model to clients and updates clients model (model_dict).\n","\n","  @param: main_model --> the server model\n","  @param: model_dict --> dictionary of all client's models\n","  @param: number_of_samples --> number of clients\n","\n","  return: model_dict: dictionary including client's model\n","  '''\n","\n","\n","  \"\"\"\n","  no_grad() function is a context manager that disables gradient calculation.\n","  This is useful for inference when you are sure you will not call tensor.\n","  backward().\n","  \"\"\"\n","  with torch.no_grad():\n","    for client_id in range(number_of_samples):\n","      # Clone the weights from the main_model to the client's model (model_dict)\n","      # model_name is only a list of the names of the models such as model0, model1\n","      model_dict[name_of_models[client_id]].load_state_dict(main_model.state_dict())\n","  return model_dict"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"oEtwRdz5kW5m"},"outputs":[],"source":["def start_train_end_node_process_print_some(number_of_samples, print_amount):\n","  '''\n","  This function starts training all clients in parallel and prints train&test accuracy, loss dictionary which includes loss for each epoch and slope.\n","\n","  @param: number_of_samples --> number of clients\n","  @param: print_amount\n","\n","  return: loss_dict,loss_lst_per_sample, slopes\n","          loss_lst_per_sample: list of client's loss per epoch\n","          loss_dict: a dictionary of the client's mean loss (we get the average from the loss list for epochs)\n","          slopes: a dictionary of the client's mean slope\n","\n","  '''\n","  loss_dict = dict()\n","  slopes = dict()\n","\n","  for i in range(number_of_samples):\n","    train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","    valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[i]], y_valid_dict[name_of_y_valid_sets[i]])\n","    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n","\n","    test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","    test_dl = DataLoader(test_ds, batch_size= batch_size)\n","    # test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n","\n","    model = model_dict[name_of_models[i]]\n","    # print_model_weights(model_dict[name_of_models[i]])\n","    # print_model_weights(model)\n","    # print(model.features[0])\n","    criterion = criterion_dict[name_of_criterions[i]]\n","    optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","    if i < print_amount:\n","      print(\"Subset\", i)\n","\n","    loss_lst_per_sample = []\n","    train_acc = []\n","    valid_acc = []\n","\n","    for epoch in range(5):\n","      train_loss_batch_list, train_accuracy, train_accuracy_skl, train_sens, train_spec, train_f1 = trainer.train_with_batch_loss(model, train_dl, criterion, optimizer)\n","      valid_loss, valid_accuracy, valid_accuracy_skl, valid_sens, valid_spec, valid_f1 =  validator.validation(model, valid_dl, criterion)\n","\n","      train_acc.append(train_accuracy)\n","      valid_acc.append(valid_accuracy)\n","      loss_lst_per_sample.append(train_loss_batch_list)\n","\n","      if i < print_amount:\n","        print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(valid_accuracy))\n","\n","    loss_lst_per_sample = list(chain.from_iterable(loss_lst_per_sample)) # flatten the list of batch losses\n","\n","    loss_dict[i] = mean(loss_lst_per_sample)\n","    slopes[i] = calculate_slope(losses=loss_lst_per_sample) # mean-slope\n","\n","    if i < print_amount:\n","      print(\"For all local epochs:\" + \" | train accuracy: {:7.5f}\".format(mean(train_acc)) + \" | test accuracy: {:7.5f}\".format(valid_accuracy))\n","      print(f\"loss_dict:{loss_dict}\")\n","      print(f\"loss_lst_per_client:{loss_lst_per_sample}\")\n","      print(f\"slope:{slopes}\")\n","\n","  return slopes"]},{"cell_type":"markdown","metadata":{"id":"cbrsDYY3kjlZ"},"source":["### Functions for Sequential Federated Averaging"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"YuMmZkcegZJq"},"outputs":[],"source":["# check data poisoning and model integrity\n","def calculate_integrity(model, baseline_model):\n","    # Calculate the L2 norm (# Calculate L2 norm like l2_norm = np.linalg.norm(v)) of the difference between the model parameters\n","    integrity = np.linalg.norm(np.array(list(model.parameters())[0].detach().numpy()) -\n","                          np.array(list(baseline_model.parameters())[0].detach().numpy()))\n","    return integrity\n","\n","def verify_model_integrity(model, baseline_model, integrity_threshold=0.01):\n","    integrity = calculate_integrity(model, baseline_model)\n","\n","    if integrity < integrity_threshold:\n","        print(f\"Model integrity check failed. Integrity: {integrity}\")\n","        return next_model\n","    else:\n","        print(f\"Model integrity check passed. Integrity: {integrity}\")\n","        return next_model.load_state_dict(model.state_dict())"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"jWI-REAWkicI"},"outputs":[],"source":["# for sequential training\n","def check_privacy_in_server_and_send_model_dict_to_next_clients(model, next_model):\n","  '''\n","  This function gets back the parameters of the previous client/sample and sets it as a parameter for the next client/sample\n","  '''\n","  with torch.no_grad():\n","    # checking model integrity, it needs to be discussed the value of integrity_threshold\n","    # verify_model_integrity(model, baseline_model, integrity_threshold=0.01)\n","\n","    # Clone the parameters from the model_dict to the next_model (next client)\n","    next_model.load_state_dict(model.state_dict())\n","\n","  return next_model"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"n16-y3d3XZBF"},"outputs":[],"source":["# model_save_dir = \"checkpoints/\"\n","# logger = Logger().get_logger(logger_name='Advertima logs')"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"nrpdxjL4rpRw"},"outputs":[],"source":["def sequential_training(model_dict, number_of_samples, sorted_samples):\n","  '''\n","  train local models sequentially\n","  clients have been reordered based on their slopes (less to more)\n","  and then we have this procedure: client1 --> server --> client2 --> server --> client3 --> ...\n","\n","  @param: main_model\n","  @param: model_dict --> dictionary of all client's models\n","  @param: number_of_samples --> number of clients\n","  @param: sorted_samples --> sorted clients based on their slopes\n","\n","  return: model --> the final model resulted from sequential learning\n","  '''\n","\n","  # a dict to save the losses of epoch * batch for reordering\n","  slopes = dict()\n","\n","  # initialize a new model\n","  next_model = base_model\n","  next_model.to(device)\n","\n","  # Initialize weights and biases to zero in the new model\n","  for param in next_model.parameters():\n","    nn.init.zeros_(param)\n","\n","  # i: a counter on clients\n","  # j: real client id\n","  for i in range(number_of_samples):\n","    if (i==0):\n","      j = sorted_samples[i][0]\n","\n","       # Clone the parameters from the model_dict to the next_model (next client)\n","      next_model.load_state_dict(model_dict[name_of_models[j]].state_dict())\n","\n","    else:\n","      j = sorted_samples[i][0]\n","\n","      # Clone the parameters from the next client to the model_dict\n","      model_dict[name_of_models[j]].load_state_dict(next_model.state_dict())\n","\n","    train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[j]], y_train_dict[name_of_y_train_sets[j]])\n","    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","    valid_ds = TensorDataset(x_valid_dict[name_of_x_valid_sets[j]], y_valid_dict[name_of_y_valid_sets[j]])\n","    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n","\n","    test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[j]], y_test_dict[name_of_y_test_sets[j]])\n","    test_dl = DataLoader(test_ds, batch_size= batch_size)\n","\n","    model = model_dict[name_of_models[j]]\n","    criterion = criterion_dict[name_of_criterions[j]]\n","    optimizer = optimizer_dict[name_of_optimizers[j]]\n","\n","    loss_lst_per_sample = []\n","    # early stopping patience; how long to wait after last time validation loss improved.\n","\n","    # initialize the early_stopping object\n","    early_stopping_local = EarlyStopping(patience=patience_local, verbose=True)\n","\n","    prev_loss = float('inf')\n","    clinet_stop_counter = 0\n","\n","    for epoch in range(numEpoch):\n","      train_loss_batch_list, train_accuracy, train_accuracy_skl, train_sens, train_spec, train_f1 = trainer.train_with_batch_loss(model, train_dl, criterion, optimizer)\n","      valid_loss, valid_accuracy, valid_accuracy_skl, valid_sens, valid_spec, valid_f1 =  validator.validation(model, valid_dl, criterion)\n","      loss_lst_per_sample.append(train_loss_batch_list)\n","\n","      # record information per ecpoch for each client training\n","      local_metrics_train.write(f\"{iter}\\t{j}\\t{train_accuracy}\\t{np.mean(train_loss_batch_list)}\\t{train_sens}\\t{train_spec}\\t{train_f1}\\n\")\n","      local_metrics_validation.write(f\"{iter}\\t{j}\\t{valid_accuracy}\\t{valid_loss}\\t{valid_sens}\\t{valid_spec}\\t{valid_f1}\\n\")\n","\n","      # testing each hospital on the same server test data for generalizability\n","      test_loss, test_accuracy, test_accuracy_skl, test_sens, test_spec, test_f1 = validator.validation(model, test_dl_server, criterion)\n","      global_metrics_test.write(f\"{iter}\\t{j}\\t{test_accuracy}\\t{test_loss}\\t{test_sens}\\t{test_spec}\\t{test_f1}\\n\")\n","\n","      '''\n","      Here we are going to stop training if the valdation loss decresses a little bit. The general trend of our validation loss is \"Descending\".\n","      So, we compare the exsiting val_loss with the previous loss and if there were not a signigicant improvment, we stop the client training.\n","            '''\n","      if valid_loss <= prev_loss:\n","        if prev_loss - valid_loss < 0.01:\n","          print(\"prev_loss - valid_loss= \", prev_loss - valid_loss)\n","          clinet_stop_counter += 1\n","\n","        if clinet_stop_counter >= patience_local:\n","            print(f\"Stoping Client {j}\")\n","            break\n","\n","        prev_loss = valid_loss\n","\n","      # early_stopping needs the validation loss to check if it has decresed,\n","      # and if it has, it will make a checkpoint of the current model\n","      early_stopping_local(valid_loss, model)\n","      if early_stopping_local.early_stop:\n","        print(\"Early Stopping Local\")\n","        break\n","\n","    loss_lst_per_sample = list(chain.from_iterable(loss_lst_per_sample)) # flatten the list of batch losses\n","    slopes[j] = calculate_slope(losses=loss_lst_per_sample) # mean-slope\n","\n","    next_model = check_privacy_in_server_and_send_model_dict_to_next_clients(model, next_model)\n","    print(f\"privacy checked and client {j} transfered its weight to server and server transfered it to next client\")\n","\n","  return model, slopes"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"KTXF1_R3uFZ8"},"outputs":[],"source":["# Function to calculate the slope of the loss function\n","def calculate_slope(losses):\n","  '''\n","  calculate the slope of the loss function\n","  '''\n","  slopes = np.diff(losses)\n","  mean_slope = np.mean(slopes)\n","  return mean_slope"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519035,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"W93uznWqAty0"},"outputs":[],"source":["def reorder_samples_wise_task_complexity(slopes):\n","  '''\n","  reorder clients wise task complexity based slopes from less to more\n","\n","  @param: slopes --> Slops is a dictionary of client id to the client slop (i.e., {c1: s1})\n","\n","  return: sorted_clients\n","  '''\n","  # For ordering\n","  sorted_clients = sorted(slopes.items(), key=lambda kv: (kv[1], kv[0]))\n","\n","  # No ordering\n","  # sorted_clients = sorted(slopes.items())\n","\n","  # Random Ordering\n","  # sorted_clients = sorted(slopes.items()) # to get the right data structure i.e., [(0, 0.1), (1, 0.2), ...]\n","  # random.shuffle(sorted_clients)\n","\n","  # sorted_clients = [(c0, s0), (c1, s1)] i.e., (k, v)\n","  return sorted_clients"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519036,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"Tr4oOAxhYYU9"},"outputs":[],"source":["def server_training(main_model, main_optimizer, main_criterion):\n","  '''\n","  train server on mixed data.\n","  '''\n","  # initialize the early_stopping object\n","  early_stopping_server = EarlyStopping(patience=patience_local, verbose=True)\n","\n","  prev_loss = float('inf')\n","  server_stop_counter = 0\n","\n","  for epoch in range(numEpoch):\n","    central_train_loss, central_train_accuracy, central_train_accuracy_skl, central_train_sens, central_train_spec, central_train_f1 = trainer.train(main_model, train_dl_server, main_criterion, main_optimizer)\n","    central_valid_loss, central_valid_accuracy, central_valid_accuracy_skl, central_valid_sens, central_valid_spec, central_valid_f1 = validator.validation(main_model, valid_dl_server, main_criterion)\n","\n","    global_metrics_train_server.write(f\"{iter}\\t{epoch}\\t{central_train_accuracy}\\t{central_train_loss}\\t{central_train_sens}\\t{central_train_spec}\\t{central_train_f1}\\n\")\n","    global_metrics_valid_server.write(f\"{iter}\\t{epoch}\\t{central_valid_accuracy}\\t{central_valid_loss}\\t{central_valid_sens}\\t{central_valid_spec}\\t{central_valid_f1}\\n\")\n","\n","    '''\n","      Here we are going to stop training if the valdation loss decresses a little bit. The general trend of our validation loss is \"Descending\".\n","      So, we compare the exsiting val_loss with the previous loss and if there were not a signigicant improvment, we stop the client training.\n","            '''\n","    if central_valid_loss <= prev_loss:\n","      if prev_loss - central_valid_loss < 0.01:\n","        print(\"prev_loss - valid_loss= \", prev_loss - central_valid_loss)\n","        server_stop_counter += 1\n","\n","      if server_stop_counter >= patience_local:\n","        print(f\"Stoping server\")\n","        break\n","\n","      prev_loss = central_valid_loss\n","\n","    # early_stopping needs the validation loss to check if it has decresed,\n","    # and if it has, it will make a checkpoint of the current model\n","    early_stopping_server(central_valid_loss, main_model)\n","    if early_stopping_server.early_stop:\n","      print(\"Early Stopping server\")\n","      break\n","\n","  print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_valid_accuracy))\n","  return main_model, main_optimizer, main_criterion"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519036,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"OZxQ5KoyQFkT"},"outputs":[],"source":["def update_accumulation(main_model, model_seq):\n","    '''\n","    Element_wise sum and average\n","\n","    An example of the result:\n","\n","    model_seqential parameters\n","    [Parameter containing:\n","    layer1.0.weight: tensor([[[[ 0.0295,  0.4453, -0.0934],\n","              [ 0.1785,  0.2598, -0.3424],\n","              [ 0.0835, -0.1489, -0.5921]]],\n","\n","\n","    Iteration 0 : main_model accuracy on server test data after training:  0.7019\n","    ****** mupdated main_model parameters\n","    [Parameter containing:\n","    layer1.0.weight: tensor([[[[-0.0799,  0.2727,  0.0540],\n","              [ 0.1563,  0.3248,  0.0287],\n","              [-0.0239,  0.0026, -0.1325]]],\n","\n","    Iteration 0 : main_model accuracy on server test data after accumulation:  0.1144\n","    ****** updated main_model parameters after accumulation\n","    [Parameter containing:\n","    layer1.0.weight: tensor([[[[-0.0252,  0.3590, -0.0197],\n","              [ 0.1674,  0.2923, -0.1568],\n","              [ 0.0298, -0.0731, -0.3623]]],\n","    '''\n","\n","    for (main_params, model_seq_params) in zip(main_model.parameters(), model_seq.parameters()):\n","      main_params.data = (a * main_params.data) + ((1-a) * model_seq_params.data)\n","    return main_model"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519036,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"KxZflGXSXZBG"},"outputs":[],"source":["def print_model_weights(model):\n","    print(\"\\n The last layer of main_model\")\n","    print(list(model.parameters())[0][1])"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1708391519036,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"h1VKngD7nW_J"},"outputs":[],"source":["# MODEL\n","learning_rate = configs['model']['learning_rate']\n","batch_size = configs['model']['batch_size']\n","momentum = configs['model']['momentum']\n","\n","numEpoch = configs['model']['num_epoch']\n","# numEpoch = 5\n","\n","NUM_ITERATION = configs['model']['num_iteration']\n","# NUM_ITERATION = 200\n","# MODEL = ['simcnn', 'cnn5', 'vgg11', 'resnet18']\n","MODEL = 'simcnn'\n","\n","# STOP = [\"wes\",\"woes\"]\n","STOP = \"woes\"\n","# For \"wes\" we have:\n","# early stopping patience; how long to wait after last time validation loss improved.\n","patience_local = 7 # wes=7  woes=7\n","patience_global = 200 # wes=10  woes=200\n","\n","STOP_CLIENTS=\"slightimprove\"\n","\n","# DATASET\n","# n_classes = 23\n","n_channels = configs['dataset']['num_channels']\n","\n","# 'seq-worder': seq using our ordering method\n","#\n","# STRATEGY = ['baseline', 'fedavg', 'seq-noorder', 'seq-worder',]\n","STRATEGY = \"seq-worder\"\n","\n","# SPILIT = ['random', 'stratified']\n","SPILT = \"stratified\"\n","\n","\n","# Status = 'withclass'\n","# num_client: is the number of all clients\n","num_client = 3*8 #24\n","number_of_samples= 3*8 #24  # 3*num_client=24\n","a = 0.7 #the mixing weight in taking average server parameters and the last client's parameters in sequential training\n","print_amount=1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1708391521661,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"MfA3URtcm7uV"},"outputs":[],"source":["def create_file():\n","\n","    local_metrics_train = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/local_train_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    local_metrics_train.write(\"iteration\\tclient_id\\ttrain_accuracy\\ttrain_loss\\ttrain_sensitivity\\ttrain_specificity\\ttrain_f1\\n\")\n","\n","    local_metrics_validation = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/local_valid_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    local_metrics_validation.write(\"iteration\\tclient_id\\tvalid_accuracy\\tvalid_loss\\tvalid_sensitivity\\tvalid_specificity\\tvalid_f1\\n\")\n","\n","    global_metrics_test = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/global_test_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    global_metrics_test.write(\"iteration\\tclient_id\\ttest_accuracy\\ttest_loss\\ttest_sensitivity\\ttest_specificity\\ttest_f1\\n\")\n","\n","    # server\n","    global_metrics_train_server = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/global_train_server_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    global_metrics_train_server.write(\"iteration\\tepoch\\ttrain_accuracy\\ttrain_loss\\ttrain_sensitivity\\ttrain_specificity\\ttrain_f1\\n\")\n","    # server\n","    global_metrics_valid_server = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/global_valid_server_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    global_metrics_valid_server.write(\"iteration\\tepoch\\tvalid_accuracy\\tvalid_loss\\tvalid_sensitivity\\tvalid_specificity\\tvalid_f1\\n\")\n","\n","    accumulate_metrics_valid = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/accumulate_valid_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    accumulate_metrics_valid.write(\"iteration\\tvalid_accuracy\\tvalid_loss\\tvalid_sensitivity\\tvalid_specificity\\tvalid_f1\\n\")\n","\n","    final_metrics_test= open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/final_test_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    final_metrics_test.write(\"iteration\\ttest_accuracy\\ttest_loss\\ttest_sensitivity\\ttest_specificity\\ttest_f1\\n\")\n","\n","    elapsed_time = open(f\"results/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/{STRATEGY}_elapsed_time_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    elapsed_time.write(\"elapsed_time\\n\")\n","\n","    return local_metrics_train, local_metrics_validation, global_metrics_test, global_metrics_train_server, global_metrics_valid_server,accumulate_metrics_valid, final_metrics_test, elapsed_time"]},{"cell_type":"markdown","metadata":{},"source":["## Ctreat these files for evaluating scalibilty/a parameter across num_clients clients "]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def create_file():\n","\n","    local_metrics_train = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/local_train_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    local_metrics_train.write(\"iteration\\tclient_id\\ttrain_accuracy\\ttrain_loss\\ttrain_sensitivity\\ttrain_specificity\\ttrain_f1\\n\")\n","\n","    local_metrics_validation = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/local_valid_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    local_metrics_validation.write(\"iteration\\tclient_id\\tvalid_accuracy\\tvalid_loss\\tvalid_sensitivity\\tvalid_specificity\\tvalid_f1\\n\")\n","\n","    global_metrics_test = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/global_test_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    global_metrics_test.write(\"iteration\\tclient_id\\ttest_accuracy\\ttest_loss\\ttest_sensitivity\\ttest_specificity\\ttest_f1\\n\")\n","\n","    # server\n","    global_metrics_train_server = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/global_train_server_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    global_metrics_train_server.write(\"iteration\\tepoch\\ttrain_accuracy\\ttrain_loss\\ttrain_sensitivity\\ttrain_specificity\\ttrain_f1\\n\")\n","    # server\n","    global_metrics_valid_server = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/global_valid_server_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    global_metrics_valid_server.write(\"iteration\\tepoch\\tvalid_accuracy\\tvalid_loss\\tvalid_sensitivity\\tvalid_specificity\\tvalid_f1\\n\")\n","\n","    accumulate_metrics_valid = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/accumulate_valid_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    accumulate_metrics_valid.write(\"iteration\\tvalid_accuracy\\tvalid_loss\\tvalid_sensitivity\\tvalid_specificity\\tvalid_f1\\n\")\n","\n","    final_metrics_test= open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/final_test_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    final_metrics_test.write(\"iteration\\ttest_accuracy\\ttest_loss\\ttest_sensitivity\\ttest_specificity\\ttest_f1\\n\")\n","\n","    elapsed_time = open(f\"results/a/{SAMPLING}/{STRATEGY}/{MODEL}/{SPILT}/{STRATEGY}_elapsed_time_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_a{a}_nc{num_client}_{STOP}_{STOP_CLIENTS}_pl{patience_local}_pg{patience_global}.txt\", 'w')\n","    elapsed_time.write(\"elapsed_time\\n\")\n","\n","    return local_metrics_train, local_metrics_validation, global_metrics_test, global_metrics_train_server, global_metrics_valid_server,accumulate_metrics_valid, final_metrics_test, elapsed_time"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708391524249,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"Z6RxoukSnM2n"},"outputs":[],"source":["local_metrics_train, local_metrics_validation, global_metrics_test, global_metrics_train_server, global_metrics_valid_server, accumulate_metrics_valid, final_metrics_test, elapsed_time = create_file()"]},{"cell_type":"markdown","metadata":{"id":"0bN9Q2hJlPCa"},"source":["### Parameter Initialization: Models, Optimizers, and Loss Functions"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708391524249,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"8VyRUp1lXZBH","outputId":"220787f4-f6d3-4b40-8118-5eb2ec10d3a5"},"outputs":[{"data":{"text/plain":["SimCNN(\n","  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=1600, out_features=2048, bias=True)\n","  (output): Linear(in_features=2048, out_features=23, bias=True)\n",")"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["base_model = get_model(model_name=MODEL)\n","base_model.to(device)\n","# base_model (to print the architecture of the model)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":6045,"status":"ok","timestamp":1708391530291,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"Q8zl7SKNlRxk"},"outputs":[],"source":["model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_samples)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1708391530295,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"wU9sbHdxmPTF"},"outputs":[],"source":["name_of_x_train_sets = list(x_train_dict.keys())\n","name_of_y_train_sets = list(y_train_dict.keys())\n","\n","name_of_x_valid_sets = list(x_valid_dict.keys())\n","name_of_y_valid_sets = list(y_valid_dict.keys())\n","\n","name_of_x_test_sets = list(x_test_dict.keys())\n","name_of_y_test_sets = list(y_test_dict.keys())\n","\n","# model_dcit = {'model_1': model_info_1, ..}\n","# names_of_models = ['model_1', 'model_2', ...]\n","name_of_models = list(model_dict.keys())\n","name_of_optimizers=list(optimizer_dict.keys())\n","name_of_criterions=list(criterion_dict.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3026146,"status":"ok","timestamp":1708394556425,"user":{"displayName":"Atefe Hassani","userId":"15686447765946134283"},"user_tz":0},"id":"iw9ug81Uvsmq","outputId":"1c63d897-bbdd-4fe6-9d06-17a6fdb7698f"},"outputs":[],"source":["# Federated learning with sequential learning strategy based on the slope of the loss function\n","print(\"Model is \", MODEL)\n","print(\"lr is \", learning_rate)\n","print(\"Num iteration is \", NUM_ITERATION)\n","print(\"Num epochs is \", numEpoch)\n","print(\"Stop\", STOP)\n","print(\"patience_local is\", patience_local)\n","print(\"patience_global is\", patience_global)\n","\n","\n","strat_training_time = time.time()\n","checkpoint_path_global = f\"checkpoints/{SAMPLING}_{STRATEGY}_{MODEL}_{SPILT}_iter{NUM_ITERATION}_epoch{numEpoch}_lr{learning_rate}_bs{batch_size}_ds{amount}_m{momentum}_{STOP}_{STOP_CLIENTS}_Global.pt\"\n","\n","# 0: creat main model\n","main_model = get_model(model_name=MODEL)\n","main_model.to(device)\n","main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=momentum)\n","main_criterion = nn.CrossEntropyLoss()\n","\n","# 1: First training run\n","print(\"First training round is started ...\")\n","model_dict = send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)\n","slopes_seq = start_train_end_node_process_print_some(number_of_samples, print_amount)\n","sorted_clients = reorder_samples_wise_task_complexity(slopes_seq)\n","print(f\"samples have been sorted by their slope values: {sorted_clients}\")\n","print(\"First training round is finished!\")\n","\n","# initialize the early_stopping object\n","early_stopping_global = EarlyStopping(patience=patience_global, verbose=True, path=checkpoint_path_global)\n","\n","for iter in range(NUM_ITERATION):\n","  print(\"global epoch\", iter)\n","\n","  # 2: Sequential training\n","  model_seq, slopes = sequential_training(model_dict, number_of_samples, sorted_clients)\n","\n","  # 3: Server training\n","  main_model, main_optimizer, main_criterion = server_training(main_model, main_optimizer, main_criterion)\n","\n","  # 4: Update accumulation\n","  main_model = update_accumulation(main_model, model_seq)\n","\n","  valid_loss_accumulation, valid_accuracy_accumulation, valid_accuracy_skl_accumulation, valid_sens_accumulation, valid_spec_accumulation, valid_f1_accumulation = validator.validation(main_model, valid_dl_server, main_criterion)\n","  accumulate_metrics_valid.write(f\"{iter}\\t{valid_accuracy_accumulation}\\t{valid_loss_accumulation}\\t{valid_sens_accumulation}\\t{valid_spec_accumulation}\\t{valid_f1_accumulation}\\n\")\n","\n","  early_stopping_global(valid_loss_accumulation, main_model)\n","  if early_stopping_global.early_stop:\n","    print(\"Early Stopping Global\")\n","    break\n","\n","  # 5: Reordering (ranking hospitals/clients)\n","  sorted_clients = reorder_samples_wise_task_complexity(slopes)\n","\n","test_loss, test_accuracy, test_accuracy_skl, test_sens, test_spec, test_f1= validator.validation(main_model, test_dl_server, main_criterion)\n","print(\"Iteration\", str(iter), \": final main_model accuracy on server test data after accumulation: {:7.4f}\".format(test_accuracy))\n","final_metrics_test.write(f\"{iter}\\t{test_accuracy}\\t{test_loss}\\t{test_sens}\\t{test_spec}\\t{test_f1}\\n\")\n","\n","\n","end_training_time = time.time()\n","elapsed_training_time = end_training_time - strat_training_time\n","print(\"elapsed_training_time:\", elapsed_training_time)\n","elapsed_time.write(f\"{elapsed_training_time}\\n\")\n","\n","local_metrics_train.close()\n","local_metrics_validation.close()\n","global_metrics_test.close()\n","global_metrics_train_server.close()\n","global_metrics_valid_server.close()\n","accumulate_metrics_valid.close()\n","final_metrics_test.close()\n","elapsed_time.close()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.-1"}},"nbformat":4,"nbformat_minor":0}
